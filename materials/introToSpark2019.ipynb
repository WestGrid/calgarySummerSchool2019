{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"http://static.ucalgary.ca/2013-001/980/global/images/identity/vertical-crest.png\"/>\n",
    "<BR><BR><BR>\n",
    "    \n",
    "# Introduction to Apache Spark\n",
    "### Instructor: Dave Schulz (Research Computing Services)\n",
    "\n",
    "An introduction to basic theory of Map-Reduce applied to Resilient Distributed Datasets (RDD). This course primarily uses PySpark to introduce the Spark approach to parallelization.\n",
    "\n",
    "This course will begin with an introduction to RDDs and MapReduce. We will demonstrate how to load data to an RDD and how to make use of the RDD API. This introduction will include practical examples of filtering, transforming, and aggregating a data stream with the RDD API. We will also introduce additional high level Spark APIs such as the DataFrame API.\n",
    "\n",
    "Target audience: researchers interested in a first introduction to parallel computations in a Spark framework\n",
    "\n",
    "Duration: 3 hours\n",
    "\n",
    "Level: beginner\n",
    "\n",
    "Prerequisites: This course assumes a familiarity with basic python syntax for variable declaration, function definition and use, and iteration.\n",
    "\n",
    "\n",
    "Laptop software: All attendees will need to bring their laptops with wireless access and with a remote SSH client installed (on Windows laptops we recommend the free edition of MobaXterm; on Mac and Linux laptops no need to install anything)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble\n",
    "\n",
    "## Load Notebook\n",
    "\n",
    "* Login to Arc with your ssh client and copy the notebook file into your homedirectory\n",
    "````shell\n",
    " ssh arc.ucalgary.ca -l <your username>\n",
    " cp /global/software/spark/SummerSchool/IntroToSpark2019.ipynb ~\n",
    "````\n",
    "* Point your browser at jupyter.ucalgary.ca and sign in with your UC account.\n",
    "* Open a copy of this notebook file from IntroToSpark2019.ipynb.\n",
    "\n",
    "## Initialize Spark \n",
    "* We'll do that early so that the helpers have time to troubleshoot while we get things started.\n",
    "* The following cell should end in something like the following (the batch job number will be different):\n",
    "> INFO:sparkhpc.sparkjob:Submitted batch job 802388\n",
    ">\n",
    "> INFO:sparkhpc.sparkjob:Submitted cluster 0\n",
    "* This code is on our website https://hpc.ucalgary.ca/arc/software/spark\n",
    "\n",
    "* Firstly check that you don't already have a spark cluster running.  The following should not have a line that mentions spark in the NAME column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID       PARTITION     NAME            USER    STATE       TIME TIME_LIMIT  TIME_LEFT START_TIME NODES CPUS NODELIST(REASON)    \r\n",
      "            879035          single jupyterh         dschulz  RUNNING       0:49    6:00:00    5:59:11 2019-05-27     1    1 cn002               \r\n"
     ]
    }
   ],
   "source": [
    "!squeue -u $USER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sparkhpc.sparkjob:Submitted batch job 879036\n",
      "\n",
      "INFO:sparkhpc.sparkjob:Submitted cluster 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import atexit\n",
    "import sys\n",
    "\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import findspark\n",
    "from sparkhpc import sparkjob\n",
    "\n",
    "#Exit handler to clean up the Spark cluster if the script exits or crashes\n",
    "def exitHandler(sj,sc):\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Context')\n",
    "        sc.stop()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Job')\n",
    "        sj.stop()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "#Parameters for the Spark cluster\n",
    "nodes=3\n",
    "tasks_per_node=8\n",
    "memory_per_task=1024 #1 gig per process, adjust accordingly\n",
    "# Please estimate walltime carefully to keep unused Spark clusters from sitting \n",
    "# idle so that others may use the resources.\n",
    "walltime=\"3:00\" #3 hours\n",
    "os.environ['SBATCH_PARTITION']='lattice' #Set the appropriate ARC partition\n",
    "\n",
    "sj = sparkjob.sparkjob(\n",
    "     ncores=nodes*tasks_per_node,\n",
    "     cores_per_executor=tasks_per_node,\n",
    "     memory_per_core=memory_per_task,\n",
    "     walltime=walltime\n",
    "    )\n",
    "\n",
    "sj.wait_to_start()\n",
    "sc = sj.start_spark()\n",
    "\n",
    "#Register the exit handler                                                                                                     \n",
    "atexit.register(exitHandler,sj,sc)\n",
    "\n",
    "#You need this line if you want to use SparkSQL\n",
    "sqlCtx=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#things we'll need later\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import operator\n",
    "%matplotlib notebook \n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Spark Journey\n",
    "* MySQL ~ weeks\n",
    " * Single server with a handful of raid disks\n",
    "* BigByte ~ day\n",
    " * 1TB ram -- put the source all in ramdisk\n",
    "* Woozle ~ 10 min\n",
    " * Very old cluster of 32 nodes 4 cores per node\n",
    " * Originally Installed in 2008\n",
    " * Work done circa 2013\n",
    "* Catalyst ~ 10 min -> <1 min with spark\n",
    " * Current system.\n",
    " * 20 nodes\n",
    " * HDFS\n",
    " * Hadoop & Spark both installed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Big Data\n",
    "* Intractable on a single machine\n",
    "* Because of:\n",
    " * disk\n",
    " * ram\n",
    " * computation time (cpu)\n",
    "* A dataset that requires parallelization to process in a reasonable amount of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"https://cdn-images-1.medium.com/max/1200/1*yWFQiGjlgHUVYeh4ELELyw.jpeg\" width=30% />\n",
    "\n",
    "# Data Cleaning\n",
    "* Missing data\n",
    "* Entity resolution\n",
    "* Unit Mismatches\n",
    "* Date formats\n",
    "* nonsense values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"https://spark.apache.org/images/spark-logo-trademark.png\" width=20% />\n",
    "\n",
    "\n",
    "# Why Apache Spark instead of Hadoop Map Reduce?\n",
    "* Short iteration cycle\n",
    "* Caching -- keep intermediate results in ram instead of writing/reading from disk\n",
    " * Can use disk if out of ram\n",
    " * The programmer can coach Spark about what needs to stay in ram\n",
    "* You can keep the data pipeline spooled up\n",
    "* Create RDD, then try things against it\n",
    "* Handles node failures gracefully \n",
    "* In Spark we setup plumbing then open the valve into the established plumbing.  \n",
    " * Since we describe what to do instead of actually doing it, we can repeat a failed task as it is just re-executing the same plumbing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# A Word on Filesystems\n",
    "* What is HDFS (Hadoop Distributed FileSystem)\n",
    "* Why do we use NFS instead of HDFS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"http://4.bp.blogspot.com/_j6mB7TMmJJY/SS0BMwJtUsI/AAAAAAAAAF4/w2YM5b1lff4/s400/P1.png\" />\n",
    "\n",
    "# What is Map-Reduce?\n",
    "* Think grep + sort + uniq\n",
    "\n",
    "## Example\n",
    "In the following text block, what is the count of words in the lines containing the word \"the\"\n",
    "\n",
    "### Input Text filename: input.txt\n",
    "\n",
    "  the quick brown fox\n",
    "  \n",
    "  jumped over\n",
    "  \n",
    "  the lazy dog. the quick\n",
    "  \n",
    "  brown fox jumped over\n",
    "  \n",
    "  the lazy dog\n",
    "\n",
    "### Answer the unix way:\n",
    "#### The Map\n",
    "1. grep lines containing \"the\"\n",
    "2. delete the period character\n",
    "2. change spaces to line breaks\n",
    "3. sort the list of words\n",
    "\n",
    "#### The Reduce\n",
    "4. uniq-count the list\n",
    "\n",
    "#### All together\n",
    "````bash\n",
    "$cat ~/input.txt | grep the | tr -d '.' | tr \" \" \"\\n\" | sort | uniq -c\n",
    "````\n",
    "````\n",
    "     1 brown\n",
    "     2 dog\n",
    "     1 fox\n",
    "     2 lazy\n",
    "     2 quick\n",
    "     4 the\n",
    "````\n",
    "## The Problem\n",
    "* Not parallelized\n",
    "\n",
    "## The Parallel solution the hard way\n",
    "1. Split the file into 2 files (around half the lines in each but it doesn't matter that much)\n",
    "1. Perform the Map and Reduce part on each file in different processes at the same time\n",
    "1. Left with two results:\n",
    " * File #1:\n",
    "````\n",
    "     1 brown\n",
    "     1 dog\n",
    "     1 fox\n",
    "     1 lazy\n",
    "     2 quick\n",
    "     3 the\n",
    "````\n",
    " * File #2:\n",
    "````\n",
    "     1 dog\n",
    "     1 lazy\n",
    "     1 the\n",
    "````\n",
    "1. Reduce them.... merge by key in column #2 (the words)\n",
    "* We avoided the ***expensive*** sort over the whole list (sorted in each separate process)\n",
    "\n",
    "## The Problem\n",
    "* Not that time consuming to setup for 2 processes but very much so with 1000's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Data Structures\n",
    "## RDD\n",
    "* Main Data structure in Spark\n",
    "* Collection of objects distributed throughout a cluster\n",
    "* Calling methods on the RDD object that are \"Transformations\" set up \"the plumbing\"\n",
    "* After Transformations are called, then \"Actions\" cause the data to flow.\n",
    "\n",
    "## DataFrames (Spark Dataframe -- not Pandas Dataframe)\n",
    "* Similar to a Pandas dataframe but parallelized across the cluster\n",
    "* Can be thought of like a table in a database\n",
    "* THE COOL PART: Can write fast SparkSQL queries against the Spark Dataframes as if they are tables in a database -- with some limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# The Hands on Part\n",
    "\n",
    "* We're going to count the words in an academic paper which is an ancestor of Spark.\n",
    "* Small example that will NOT illustrate scaling\n",
    "\n",
    "## Confirm Spark is Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "print(sc.version)\n",
    "assert(sc.version=='2.4.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load some Data\n",
    "* Let's take a historical Hadoop paper. This paper is rumoured to be what started hadoop. https://ai.google/research/pubs/pub62.pdf\n",
    "\n",
    "* The paper has been converted from pdf to text and left in /global/software/spark/SummerSchool/pub62.txt\n",
    "* The following cell creates an RDD from the lines of the pub62.txt file.\n",
    "* The take() function is an action which realizes the \"plumbing\" which was setup.\n",
    "* The sc.textFile function does very little and doesn't even load the file.  It just connects the RDD's input to that file waiting for an action to tell it to start reading.\n",
    "* The take() function is the action that tells Spark to read from it's input until 10 lines/items are produced at the output. -- Similar to the Unix tail program or a LIMIT clause in a SQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Lines in RDD:1296\n",
      "First few lines:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['MapReduce: Simplified Data Processing on Large Clusters',\n",
       " 'Jeffrey Dean and Sanjay Ghemawat',\n",
       " 'jeff@google.com, sanjay@google.com',\n",
       " '',\n",
       " 'Google, Inc.',\n",
       " '',\n",
       " 'Abstract',\n",
       " 'MapReduce is a programming model and an associated implementation for processing and generating large',\n",
       " 'data sets. Users specify a map function that processes a',\n",
       " 'key/value pair to generate a set of intermediate key/value']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hadoopPaperRDD=sc.textFile('/global/software/spark/SummerSchool/pub62.txt')\n",
    "print(\"Number of Lines in RDD:%i\" % hadoopPaperRDD.count())\n",
    "print(\"First few lines:\")\n",
    "hadoopPaperRDD.take(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the Data\n",
    "* Not a complete cleaning - simplified for illustration purposes\n",
    "* For the first part, we'll use a standard function, and for the second part we'll use a lambda function\n",
    " * Delete all non alphabet or space characters\n",
    " * Lowercase all characters\n",
    " * split on spaces\n",
    " * Should have rows with a list of all words in each line -- will have repeats\n",
    "* We'll do the above with a map function.  \n",
    "* First we write the function and test it, then Spark will take care of copying the function to the other nodes.  \n",
    "* We can run the function here in the notebook as it is an ordinary function.\n",
    "## Things We'll Need\n",
    "* re.sub -- Regular Expression Substitute https://docs.python.org/2/library/re.html#re.sub\n",
    "* lower() -- Return the lowercase version of a string https://docs.python.org/3/library/stdtypes.html#str.lower\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['MapReduce: Simplified Data Processing on Large Clusters']\n",
      " After: mapreduce simplified data processing on large clusters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def cleanRow(row):\n",
    "    \n",
    "    # Delete all non alphanumeric characters\n",
    "    rowAlnum=re.sub('[^a-zA-Z0-9 ]+','', row)\n",
    "    \n",
    "    # Lowercase the entire string\n",
    "    rowLowercase=rowAlnum.lower()\n",
    "    \n",
    "    return rowLowercase\n",
    "\n",
    "\n",
    "\n",
    "## Test the Above Function\n",
    "\n",
    "testRow=hadoopPaperRDD.take(1)\n",
    "print(\"Before: %s\" % testRow)\n",
    "print(\" After: %s\" % cleanRow(str(hadoopPaperRDD.take(1))))\n",
    "assert(cleanRow(str(hadoopPaperRDD.take(1))) == 'mapreduce simplified data processing on large clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply the cleanRow() Function to the Rows of the RDD\n",
    "## Things We'll Need\n",
    "* map() - Map the values in the RDD https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.map\n",
    "* take() - Return the first (x) values unordered https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mapreduce simplified data processing on large clusters',\n",
       " 'jeffrey dean and sanjay ghemawat',\n",
       " 'jeffgooglecom sanjaygooglecom',\n",
       " '',\n",
       " 'google inc',\n",
       " '',\n",
       " 'abstract',\n",
       " 'mapreduce is a programming model and an associated implementation for processing and generating large',\n",
       " 'data sets users specify a map function that processes a',\n",
       " 'keyvalue pair to generate a set of intermediate keyvalue']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hadoopPaperRDD_lowercased=hadoopPaperRDD.map(cleanRow)\n",
    "hadoopPaperRDD_lowercased.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's Split on Spaces\n",
    "* We'll use a regex to split the rows on spaces and a new function called flatMap\n",
    "### Things We'll Need\n",
    "* flatMap() -- Map values but return separate list elements as separate records https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.flatMap\n",
    "* take() - Return the first (x) values unordered https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mapreduce',\n",
       " 'simplified',\n",
       " 'data',\n",
       " 'processing',\n",
       " 'on',\n",
       " 'large',\n",
       " 'clusters',\n",
       " 'jeffrey',\n",
       " 'dean',\n",
       " 'and']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hadoopPaperRDD_split=hadoopPaperRDD_lowercased.flatMap(lambda x: re.split(' ', x))\n",
    "hadoopPaperRDD_split.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result: List of Words\n",
    "* The result is a list of words in roughly the same order they were in the original file.\n",
    "* Still need to count distinct words\n",
    "* need a key-value pair list of the form word->count\n",
    "* Something un-obvious to be done here, set each word->1 and combine in the reducer\n",
    "## Things We'll Need\n",
    "* map() - Map the values in the RDD https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.map\n",
    "* take() - Return the first (x) values unordered https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mapreduce', 1),\n",
       " ('simplified', 1),\n",
       " ('data', 1),\n",
       " ('processing', 1),\n",
       " ('on', 1),\n",
       " ('large', 1),\n",
       " ('clusters', 1),\n",
       " ('jeffrey', 1),\n",
       " ('dean', 1),\n",
       " ('and', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordListUnmergedRDD=hadoopPaperRDD_split.map(lambda x: (x,1))\n",
    "wordListUnmergedRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the Keys\n",
    "* we have something like this:\n",
    " * keyA=1\n",
    " * keyA=1\n",
    " * keyB=1\n",
    " * keyC=1\n",
    " * keyA=1\n",
    "* We want this:\n",
    " * keyA=3\n",
    " * keyB=1\n",
    " * keyC=1\n",
    "* To do this we use the function reduceByKey()\n",
    "## Things We'll Need\n",
    "* reduceByKey() - Merge the values for each key using an associative, commutative function https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.reduceByKey\n",
    "* count() - Count the number of values returned by the RDD https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.count\n",
    "* take() - Return the first (x) values unordered https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record count=1781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('particular', 5),\n",
       " ('record', 4),\n",
       " ('skipped', 1),\n",
       " ('when', 25),\n",
       " ('issues', 3),\n",
       " ('reexecution', 6),\n",
       " ('of', 338),\n",
       " ('reduce', 91),\n",
       " ('task', 50),\n",
       " ('', 290)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "wordListMergedRDD=wordListUnmergedRDD.reduceByKey(operator.add)\n",
    "print(\"Record count=%i\"%wordListMergedRDD.count())\n",
    "wordListMergedRDD.take(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WAIT! '' is NOT a word!  Let's remove it\n",
    "* While we're at it, maybe we don't like works <= 2 characters\n",
    "* Or words that start with a digit (probably numbers)\n",
    "## Things We'll Need\n",
    "* reduceByKey() - Merge the values for each key using an associative, commutative function https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.reduceByKey\n",
    "* filter() - Return an RDD with elements that satisfy the condition https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.filter\n",
    "* take() - Return the first (x) values unordered https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record count=1595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('simplified', 1),\n",
       " ('large', 32),\n",
       " ('clusters', 5),\n",
       " ('jeffrey', 2),\n",
       " ('sanjay', 2),\n",
       " ('sanjaygooglecom', 1),\n",
       " ('google', 14),\n",
       " ('inc', 1),\n",
       " ('abstract', 1),\n",
       " ('programming', 14)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordListMergedRDD=(\n",
    "    wordListUnmergedRDD\n",
    "    .filter(lambda x: re.match('^\\D', x[0])) #Remove starts with a digit\n",
    "    .filter(lambda x: x[0]!='')              #Remove empty string\n",
    "    .filter(lambda x: len(x[0]) > 2)         #Remove showrt words <=2 characters\n",
    "    .reduceByKey(operator.add)               #Count all the words\n",
    ")\n",
    "print(\"Record count=%i\"%wordListMergedRDD.count())\n",
    "wordListMergedRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's sort this a couple of ways\n",
    "## Things We'll Need\n",
    "* sortByKey() - Sort the RDD by the Key https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.sortByKey\n",
    "* sortBy() - Sort by something else based on a function written by you https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.sortBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('able', 2),\n",
       " ('abort', 1),\n",
       " ('aborts', 1),\n",
       " ('about', 12),\n",
       " ('above', 4),\n",
       " ('abstract', 1),\n",
       " ('abstraction', 2),\n",
       " ('abstractions', 1),\n",
       " ('acceptable', 1),\n",
       " ('accepts', 2)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordListMergedRDD.sortByKey().take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 611),\n",
       " ('and', 194),\n",
       " ('for', 112),\n",
       " ('mapreduce', 93),\n",
       " ('reduce', 91),\n",
       " ('data', 82),\n",
       " ('that', 80),\n",
       " ('map', 79),\n",
       " ('output', 67),\n",
       " ('tasks', 56)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordListMergedRDD.sortBy(lambda x: x[1], ascending=False).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Word First Letters\n",
    "## We need a map function\n",
    "* Let's use a real function instead of a lambda function for this\n",
    "* Allows multi-line functions\n",
    "* Can NOT share data between mappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 100)\n"
     ]
    }
   ],
   "source": [
    "def getFirstLetter(kvp):\n",
    "    a=kvp[0][0]\n",
    "    b=kvp[1]\n",
    "    return (a,b)\n",
    "print(getFirstLetter(('abc', 100)))\n",
    "assert (getFirstLetter(('abc', 100)) == ('a', 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 542),\n",
       " ('b', 139),\n",
       " ('c', 389),\n",
       " ('d', 286),\n",
       " ('e', 270),\n",
       " ('f', 450),\n",
       " ('g', 103),\n",
       " ('h', 149),\n",
       " ('i', 300),\n",
       " ('j', 35),\n",
       " ('k', 70),\n",
       " ('l', 186),\n",
       " ('m', 490),\n",
       " ('n', 123),\n",
       " ('o', 292),\n",
       " ('p', 452),\n",
       " ('q', 6),\n",
       " ('r', 323),\n",
       " ('s', 571),\n",
       " ('t', 1121),\n",
       " ('u', 163),\n",
       " ('v', 57),\n",
       " ('w', 281),\n",
       " ('x', 2),\n",
       " ('y', 6),\n",
       " ('z', 5)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstLetterFrequency=(\n",
    "    wordListMergedRDD\n",
    "    .filter(lambda x: len(x[0])>0)\n",
    "    .map(getFirstLetter)\n",
    "    .reduceByKey(operator.add)\n",
    "    .sortByKey()\n",
    ").collect()\n",
    "firstLetterFrequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'Initial Letter')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "xlabels=[x[0] for x in firstLetterFrequency]\n",
    "yvalues=[x[1] for x in firstLetterFrequency]\n",
    "plt.bar(xlabels, yvalues)\n",
    "plt.title(\"Word Initial Character Frequency\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Initial Letter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Something Different\n",
    "* Example from the Apache Spark documentation\n",
    "* Calculating Pi by the ratio of random points in the unit square at the origin that fall within the 1/4 unit circle (yellow) and outside of it (red) using \n",
    "\\begin{equation*}\n",
    "x^2+y^2<1\n",
    "\\end{equation*}\n",
    "* Then multiply by 4 to get all 4 quadrants\n",
    "<img src=\"http://www.oxfordmathcenter.com/images/notes/62-00.jpg\" />\n",
    "\n",
    "* First on a single cpu, then on all the cpus in our spark cluster and a job that is 12 times bigger to make it take long enough!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.141677\n",
      "CPU times: user 49.4 s, sys: 5.03 ms, total: 49.5 s\n",
      "Wall time: 49.6 s\n"
     ]
    }
   ],
   "source": [
    "def lineartest():\n",
    "    import random\n",
    "    NUM_SAMPLES=100000000\n",
    "    inside=0\n",
    "    for i in range(0, NUM_SAMPLES):\n",
    "        x=random.random()\n",
    "        y=random.random()\n",
    "        if (x*x) + (y*y) < 1:\n",
    "            inside=inside+1\n",
    "    print(\"Pi is roughly %f\" % (4.0 * inside / NUM_SAMPLES))\n",
    "%time lineartest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.141593\n",
      "CPU times: user 13.6 ms, sys: 6 ms, total: 19.6 ms\n",
      "Wall time: 50.9 s\n"
     ]
    }
   ],
   "source": [
    "def paralleltest():\n",
    "    NUM_SAMPLES=100000000*12\n",
    "    def inside(p):\n",
    "        x, y = random.random(), random.random()\n",
    "        return x*x + y*y < 1\n",
    "\n",
    "    count = sc.parallelize(range(0, NUM_SAMPLES)) \\\n",
    "                 .filter(inside).count()\n",
    "    print(\"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES))\n",
    "%time paralleltest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Dataframes -- Think SQL\n",
    "* Actually an implementation of HiveQL over tables of data\n",
    "* Dataframes can be treated as SQL tables or as RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Some Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Output:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9164032197314876, 0.18204743177149274, True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generateRow(_=0):\n",
    "    x=random.random()\n",
    "    y=random.random()\n",
    "    return (x,\n",
    "            y,\n",
    "            x*x + y*y < 1\n",
    "           )\n",
    "print(\"Example Output:\")\n",
    "generateRow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES=100000000*12\n",
    "# NUM_SAMPLES=20\n",
    "generatedDataRDD = sc.parallelize(range(0, NUM_SAMPLES)).map(generateRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "generatedDataDF = generatedDataRDD.toDF(schema=['X','Y','isInside'])\n",
    "generatedDataDF.registerTempTable(\"points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell already done for you.  It writes the contents of generatedDataDF to disk \n",
    "# in an efficient format.  It will fail to run for you because your account will not have\n",
    "# permission to write to /scratch/deschulz/.  Change the path to somewhere in your home\n",
    "# directory if you want to try this.  It creates ~20GB of data so your quota needs to be\n",
    "# large enough to handle all that data.\n",
    "\n",
    "#!rm -rf /scratch/dschulz/randomdataDF.parquet\n",
    "# generatedDataDF.take(10)\n",
    "#generatedDataDF.write.parquet('/scratch/dschulz/randomdataDF.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS\r\n",
      "part-00000-51891231-a5eb-4e64-8981-791c1612ecd0-c000.snappy.parquet\r\n",
      "part-00001-51891231-a5eb-4e64-8981-791c1612ecd0-c000.snappy.parquet\r\n",
      "part-00002-51891231-a5eb-4e64-8981-791c1612ecd0-c000.snappy.parquet\r\n",
      "part-00003-51891231-a5eb-4e64-8981-791c1612ecd0-c000.snappy.parquet\r\n",
      "part-00004-51891231-a5eb-4e64-8981-791c1612ecd0-c000.snappy.parquet\r\n",
      "part-00005-51891231-a5eb-4e64-8981-791c1612ecd0-c000.snappy.parquet\r\n",
      "part-00006-51891231-a5eb-4e64-8981-791c1612ecd0-c000.snappy.parquet\r\n",
      "part-00007-51891231-a5eb-4e64-8981-791c1612ecd0-c000.snappy.parquet\r\n",
      "part-00008-51891231-a5eb-4e64-8981-791c1612ecd0-c000.snappy.parquet\r\n",
      "part-00009-51891231-a5eb-4e64-8981-791c1612ecd0-c000.snappy.parquet\r\n",
      "part-00010-51891231-a5eb-4e64-8981-791c1612ecd0-c000.snappy.parquet\r\n",
      "part-00011-51891231-a5eb-4e64-8981-791c1612ecd0-c000.snappy.parquet\r\n",
      "part-00012-51891231-a5eb-4e64-8981-791c1612ecd0-c000.snappy.parquet\r\n",
      "part-00013-51891231-a5eb-4e64-8981-791c1612ecd0-c000.snappy.parquet\r\n",
      "part-00014-51891231-a5eb-4e64-8981-791c1612ecd0-c000.snappy.parquet\r\n",
      "part-00015-51891231-a5eb-4e64-8981-791c1612ecd0-c000.snappy.parquet\r\n",
      "part-00016-51891231-a5eb-4e64-8981-791c1612ecd0-c000.snappy.parquet\r\n",
      "part-00017-51891231-a5eb-4e64-8981-791c1612ecd0-c000.snappy.parquet\r\n",
      "part-00018-51891231-a5eb-4e64-8981-791c1612ecd0-c000.snappy.parquet\r\n",
      "part-00019-51891231-a5eb-4e64-8981-791c1612ecd0-c000.snappy.parquet\r\n",
      "part-00020-51891231-a5eb-4e64-8981-791c1612ecd0-c000.snappy.parquet\r\n",
      "part-00021-51891231-a5eb-4e64-8981-791c1612ecd0-c000.snappy.parquet\r\n",
      "part-00022-51891231-a5eb-4e64-8981-791c1612ecd0-c000.snappy.parquet\r\n",
      "part-00023-51891231-a5eb-4e64-8981-791c1612ecd0-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!ls /scratch/dschulz/randomdataDF.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19G\t/scratch/dschulz/randomdataDF.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!du -hs /scratch/dschulz/randomdataDF.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Parquet data file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "generatedDataDF=sqlCtx.read.parquet('/scratch/dschulz/randomdataDF.parquet')\n",
    "sqlCtx.registerDataFrameAsTable(generatedDataDF, 'points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a SQL Query\n",
    "* Against the table we just loaded\n",
    "* We'll print the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.35 ms, sys: 5.98 ms, total: 7.33 ms\n",
      "Wall time: 4.41 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(X=0.7653002042914143, Y=0.42797580223527776, isInside=True),\n",
       " Row(X=0.5700557009860996, Y=0.900421513494445, isInside=False),\n",
       " Row(X=0.5288054565478099, Y=0.958931808257132, isInside=False),\n",
       " Row(X=0.7740213455219319, Y=0.47382977304623386, isInside=True),\n",
       " Row(X=0.5731820364443511, Y=0.3043384374719159, isInside=True),\n",
       " Row(X=0.6191027515107765, Y=0.38508545105889125, isInside=True),\n",
       " Row(X=0.7506836197637178, Y=0.22221542359621882, isInside=True),\n",
       " Row(X=0.6027432039612445, Y=0.32073048277957705, isInside=True),\n",
       " Row(X=0.956487328648317, Y=0.19926002266095444, isInside=True),\n",
       " Row(X=0.9293192958185261, Y=0.8875949800654069, isInside=False)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time sqlCtx.sql(\"SELECT * FROM points WHERE X > 0.5\").take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.32 ms, sys: 1.01 ms, total: 5.33 ms\n",
      "Wall time: 7.93 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(rowCount=942490407, isInside=True),\n",
       " Row(rowCount=257509593, isInside=False)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time result=sqlCtx.sql(\"SELECT count(*) as rowCount, isInside from points group by isInside\").take(100)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.141635\n"
     ]
    }
   ],
   "source": [
    "InsideCount=result[0][0]\n",
    "OutsideCount=result[1][0]\n",
    "TotalCount=InsideCount+OutsideCount\n",
    "print(\"Pi is roughly %f\" % (4*InsideCount/TotalCount) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sparkhpc.sparkjob:\n"
     ]
    }
   ],
   "source": [
    "sc.stop()\n",
    "sj.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check job status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID       PARTITION     NAME            USER    STATE       TIME TIME_LIMIT  TIME_LEFT START_TIME NODES CPUS NODELIST(REASON)    \r\n",
      "            879035          single jupyterh         dschulz  RUNNING       5:15    6:00:00    5:54:45 2019-05-27     1    1 cn002               \r\n"
     ]
    }
   ],
   "source": [
    "!squeue -u $USER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
